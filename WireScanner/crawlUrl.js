import { getRssFeeds, testSupabaseConnection, articleExists, insertArticle } from './supabaseUtils.js';
import { parseFeed, isValidUrl } from './rssUtils.js';
import { sendWebhook } from './webHook.js';
import fs from 'fs';
import path from 'path';
import dotenv from 'dotenv';

// Charge le fichier .env situ√© dans le dossier parent
dotenv.config({ path: '../key.env' });

const LOG_FILE = path.join(process.cwd(), 'cron-task.log');

function logToFile(msg) {
    const ts = new Date().toISOString();
    const line = `[${ts}] ${msg}\n`;
    fs.appendFileSync(LOG_FILE, line);
    console.log(line.trim());
}

/**
 * Fonction principale pour crawler les flux RSS et ins√©rer les nouveaux articles dans la base Supabase.
 * - R√©cup√®re la liste des flux RSS depuis Supabase
 * - Pour chaque flux, parse les articles
 * - Ins√®re les nouveaux articles non pr√©sents dans la base
 */
export async function crawlUrl() {
    // R√©cup√©ration des URLs de flux RSS depuis la table ListUrlRss de Supabase
    const sources = await getRssFeeds();
    // V√©rification de la connexion √† Supabase
    await testSupabaseConnection();

    let totalArticlesInserted = 0; // Compteur pour les articles ins√©r√©s

    // Parcours de chaque source RSS r√©cup√©r√©e depuis la base
    for (const source of sources) {
        // V√©rification de la validit√© de l'URL
        if (!isValidUrl(source.url)) {
            console.warn(`‚ö†Ô∏è URL invalide ignor√©e : ${source.url}`);
            continue;
        }

        console.log(`üì• Lecture de ${source.url}`);
        try {
            // Parsing du flux RSS
            const feed = await parseFeed(source.url);
            // Parcours de chaque article du flux
            for (const item of feed.items) {
                const articleUrl = item.link;
                // V√©rifie que l'URL de l'article est valide
                if (articleUrl && isValidUrl(articleUrl)) {
                    try {
                        // V√©rifie si l'article existe d√©j√† dans la base
                        if (!(await articleExists(articleUrl))) {
                            // Insertion de l'article s'il n'existe pas
                            await insertArticle(articleUrl);
                            totalArticlesInserted++; // Incr√©mente le compteur
                            console.log(`‚úÖ Article ins√©r√© : ${articleUrl}`);
                        } else {
                            // L'article existe d√©j√†
                            console.log(`üîÅ Article d√©j√† pr√©sent : ${articleUrl}`);
                        }
                    } catch (err) {
                        // Affiche une erreur si la v√©rification ou l'insertion √©choue
                        console.error(err.message);
                    }
                }
            }
        } catch (err) {
            // Affiche un avertissement si le parsing du flux √©choue
            console.warn(`‚ùå Erreur lors du traitement du flux : ${source.url}`, err.message);
        }
    }

    // Log de fin de scrapping
    logScrapingCompletion(sources.length, totalArticlesInserted);
}


/**
 * D√©tecte la fin du scrapping et log un message de confirmation.
 * @param {number} totalSources - Nombre total de sources trait√©es
 * @param {number} totalArticles - Nombre total d'articles ins√©r√©s
 */
function logScrapingCompletion(totalSources, totalArticles) {
    const msg = `‚úÖ Scrapping termin√© : ${totalSources} sources trait√©es, ${totalArticles} articles ins√©r√©s.`;
    logToFile(msg);
    // Envoi d'un webhook √† la fin du scrapping
    const urlWebHook = process.env.SCRAPING_WEBHOOK_URL;
    if (urlWebHook) {
        sendWebhook(urlWebHook, {
            event: 'scraping_completed',
            sources: totalSources,
            articles: totalArticles,
            timestamp: new Date().toISOString()
        }).then(() => {
            logToFile('üì° Webhook envoy√© avec succ√®s.');
        }).catch(err => {
            logToFile('‚ùå Erreur lors de l‚Äôenvoi du webhook : ' + err.message);
        });
    }
}

// Test manuel de la fonction logScrapingCompletion
// Commande : TEST_LOG_SCRAPING_COMPLETION=1 bun run WireScanner/crawlUrl.js
if (process.env.TEST_LOG_SCRAPING_COMPLETION === '1') {
    logScrapingCompletion(2, 10); // exemple : 2 sources, 10 articles
}

