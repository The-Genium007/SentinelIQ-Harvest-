import { getRssFeeds, testSupabaseConnection, articleExists, insertArticle } from './supabaseUtils.js';
import { parseFeed, isValidUrl } from './rssUtils.js';
import { sendWebhook } from './webHook.js';
import fs from 'fs';
import path from 'path';
import dotenv from 'dotenv';

// Charge le fichier .env situÃ© dans le dossier parent
dotenv.config({ path: '../key.env' });

const LOG_FILE = path.join(process.cwd(), 'cron-task.log');

function logToFile(msg) {
    const ts = new Date().toISOString();
    const line = `[${ts}] ${msg}\n`;
    fs.appendFileSync(LOG_FILE, line);
    console.log(line.trim());
}

/**
 * Fonction principale pour crawler les flux RSS et insÃ©rer les nouveaux articles dans la base Supabase.
 * - RÃ©cupÃ¨re la liste des flux RSS depuis Supabase
 * - Pour chaque flux, parse les articles
 * - InsÃ¨re les nouveaux articles non prÃ©sents dans la base
 */
export async function crawlUrl() {
    // RÃ©cupÃ©ration des URLs de flux RSS depuis la table ListUrlRss de Supabase
    const sources = await getRssFeeds();
    // VÃ©rification de la connexion Ã  Supabase
    await testSupabaseConnection();

    let totalArticlesInserted = 0; // Compteur pour les articles insÃ©rÃ©s

    // Parcours de chaque source RSS rÃ©cupÃ©rÃ©e depuis la base
    for (const source of sources) {
        // VÃ©rification de la validitÃ© de l'URL
        if (!isValidUrl(source.url)) {
            console.warn(`âš ï¸ URL invalide ignorÃ©e : ${source.url}`);
            continue;
        }

        console.log(`ğŸ“¥ Lecture de ${source.url}`);
        try {
            // Parsing du flux RSS
            const feed = await parseFeed(source.url);
            // Parcours de chaque article du flux
            for (const item of feed.items) {
                const articleUrl = item.link;
                // VÃ©rifie que l'URL de l'article est valide
                if (articleUrl && isValidUrl(articleUrl)) {
                    try {
                        // VÃ©rifie si l'article existe dÃ©jÃ  dans la base
                        if (!(await articleExists(articleUrl))) {
                            // Insertion de l'article s'il n'existe pas
                            await insertArticle(articleUrl);
                            totalArticlesInserted++; // IncrÃ©mente le compteur
                            console.log(`âœ… Article insÃ©rÃ© : ${articleUrl}`);
                        } else {
                            // L'article existe dÃ©jÃ 
                            console.log(`ğŸ” Article dÃ©jÃ  prÃ©sent : ${articleUrl}`);
                        }
                    } catch (err) {
                        // Affiche une erreur si la vÃ©rification ou l'insertion Ã©choue
                        console.error(err.message);
                    }
                }
            }
        } catch (err) {
            // Affiche un avertissement si le parsing du flux Ã©choue
            console.warn(`âŒ Erreur lors du traitement du flux : ${source.url}`, err.message);
        }
    }

    // Log de fin de scrapping
    logScrapingCompletion(sources.length, totalArticlesInserted);
}


/**
 * DÃ©tecte la fin du scrapping et log un message de confirmation.
 * @param {number} totalSources - Nombre total de sources traitÃ©es
 * @param {number} totalArticles - Nombre total d'articles insÃ©rÃ©s
 */
function logScrapingCompletion(totalSources, totalArticles) {
    const msg = `âœ… Scrapping terminÃ© : ${totalSources} sources traitÃ©es, ${totalArticles} articles insÃ©rÃ©s.`;
    logToFile(msg);
    // Envoi d'un webhook Ã  la fin du scrapping
    const urlWebHook = process.env.SCRAPING_WEBHOOK_URL;
    if (urlWebHook) {
        sendWebhook(urlWebHook, {
            event: 'scraping_completed',
            sources: totalSources,
            articles: totalArticles,
            timestamp: new Date().toISOString()
        }).then(() => {
            logToFile('ğŸ“¡ Webhook envoyÃ© avec succÃ¨s.');
        }).catch(err => {
            logToFile('âŒ Erreur lors de lâ€™envoi du webhook : ' + err.message);
        });
    }
}
